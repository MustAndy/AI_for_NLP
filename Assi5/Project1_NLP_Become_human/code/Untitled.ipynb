{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\aicourse\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "d:\\anaconda3\\envs\\aicourse\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file found ! Now loading!\n",
      "['先生', '表示', '中新网6月23日电 (记者潘旭临) 意大利航空首席商务官乔治先生22日在北京接受中新网记者专访时表示，意航确信中国市场对意航的重要性，目前意航已将发展中国市场提升到战略层级的高度，未来，意航将加大在华布局，提升业务水平。']\n",
      "['乔治', '称', '乔治称，随着对华业务不断提升，意航明年可能会将每周4班提高到每天一班。']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "from pyltp import SementicRoleLabeller\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import re\n",
    "import jieba\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "# import processHandler\n",
    "import pyltp\n",
    "\n",
    "# pyltp官方文档http://pyltp.readthedocs.io/zh_CN/develop/api.html#id15\n",
    "# http://blog.csdn.net/MebiuW/article/details/52496920\n",
    "# http://blog.csdn.net/lalalawxt/article/details/55804384\n",
    "DATA_PATH = 'd:/senior/aiCourse/dataSource/'\n",
    "LTP_DATA_DIR = 'D://senior/aiCourse/dataSource/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')\n",
    "\n",
    "# 分句，也就是将一片文本分割为独立的句子\n",
    "def sentence_splitter(news='你好，你觉得这个例子从哪里来的？'):\n",
    "    news = news.replace('\\n','')\n",
    "    sents = SentenceSplitter.split(news)  # 分句\n",
    "    SentenceSplitter\n",
    "    #print('\\n'.join(sents))\n",
    "    return sents\n",
    "\n",
    "def split_sents( content):\n",
    "    return [sentence for sentence in re.split(r'[？?！!。；;：:\\n\\r]', content) if sentence]\n",
    "# 分词\n",
    "def segmentor(sentence=None):\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(cws_model_path)  # 加载模型\n",
    "    words = segmentor.segment(sentence)  # 分词\n",
    "    # 转换成List 输出\n",
    "    words_list = list(words)\n",
    "    segmentor.release()  # 释放模型\n",
    "    return words_list\n",
    "#词性标注\n",
    "def posttagger(words):\n",
    "    postagger = Postagger()  # 初始化实例\n",
    "    postagger.load(pos_model_path)  # 加载模型\n",
    "    postags = postagger.postag(words)  # 词性标注\n",
    "    # for word, tag in zip(words, postags):\n",
    "    #     print(word + '/' + tag)\n",
    "    postagger.release()  # 释放模型\n",
    "    return postags\n",
    "\n",
    "#命名实体\n",
    "def ner(words, postags):\n",
    "    recognizer = NamedEntityRecognizer()\n",
    "    recognizer.load(ner_model_path)  # 加载模型\n",
    "    netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "    # for word, ntag in zip(words, netags):\n",
    "    #     print(word + '/' + ntag)\n",
    "    recognizer.release()  # 释放模型\n",
    "    nerttags = list(netags)\n",
    "    return nerttags\n",
    "\n",
    "#依存句法分析\n",
    "def parse(words, postags):\n",
    "    parser = Parser()  # 初始化实例\n",
    "    parser.load(par_model_path)  # 加载模型\n",
    "    arcs = parser.parse(words, postags)  # 句法分析\n",
    "    # print(\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "    parser.release()  # 释放模型\n",
    "    return arcs\n",
    "\n",
    "# # 语义角色标注\n",
    "def role_label(words, postags,arcs):\n",
    "    labeller = SementicRoleLabeller()  # 初始化实例\n",
    "    labeller.load(srl_model_path)  # 加载模型\n",
    "    roles = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "    for role in roles:\n",
    "        print(role.index + \"\".join(\n",
    "            [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "    labeller.release()  # 释放模型\n",
    "    return roles\n",
    "\n",
    "def load_say_word():\n",
    "    say_word_file= os.path.join(DATA_PATH,'say.pickle')\n",
    "    if os.path.exists(say_word_file):  \n",
    "        print('Pickle file found ! Now loading!')\n",
    "        result = pickle.load(open(say_word_file,'rb'))\n",
    "    else:\n",
    "        print('Pickle file not found ! Starting searching, please wait for about 2 minutes.')\n",
    "        related_words = get_related_words(['说', '表示'], all_word2vec)\n",
    "        pickle.dump(related_words,open(say_word_file,'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "        result = pickle.load(open(say_word_file,'rb'))\n",
    "\n",
    "    return result\n",
    "\n",
    "shuo_dict = load_say_word()\n",
    "\n",
    "def build_parse_child_dict(words, postags, arcs):\n",
    "    child_dict_list = []\n",
    "    format_parse_list = []\n",
    "    for index in range(len(words)):\n",
    "        child_dict = dict()\n",
    "        for arc_index in range(len(arcs)):\n",
    "            if arcs[arc_index].head == index + 1:  # arcs的索引从1开始\n",
    "                if arcs[arc_index].relation in child_dict:\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                else:\n",
    "                    child_dict[arcs[arc_index].relation] = []\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "        child_dict_list.append(child_dict)\n",
    "    rely_id = [arc.head for arc in arcs]  # 提取依存父节点id\n",
    "    relation = [arc.relation for arc in arcs]  # 提取依存关系\n",
    "    heads = ['Root' if id == 0 else words[id - 1] for id in rely_id]  # 匹配依存父节点词语\n",
    "    for i in range(len(words)):\n",
    "        # ['ATT', '李克强', 0, 'nh', '总理', 1, 'n']\n",
    "        a = [relation[i], words[i], i, postags[i], heads[i], rely_id[i] - 1, postags[rely_id[i] - 1]]\n",
    "        format_parse_list.append(a)\n",
    "    return  child_dict_list, format_parse_list\n",
    "\n",
    "'''对找出的主语或者宾语进行扩展'''\n",
    "def complete_e( words, postags, child_dict_list, word_index):\n",
    "    child_dict = child_dict_list[word_index]\n",
    "    prefix = ''\n",
    "    if 'ATT' in child_dict:\n",
    "        for i in range(len(child_dict['ATT'])):\n",
    "            prefix += complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "    postfix = ''\n",
    "    if postags[word_index] == 'v':\n",
    "        if 'VOB' in child_dict:\n",
    "            postfix += complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "        if 'SBV' in child_dict:\n",
    "            prefix = complete_e(words, postags, child_dict_list, child_dict['SBV'][0]) + prefix\n",
    "\n",
    "    return prefix + words[word_index] + postfix\n",
    "\n",
    "#\n",
    "def get_sbv1_(corpus,dict):\n",
    "    words = segmentor(corpus)\n",
    "    postags = posttagger(words)\n",
    "    arcs = parse(words,postags)\n",
    "    #\n",
    "    rely_id = [arc.head for arc in arcs]  # 提取依存父节点id\n",
    "    relation = [arc.relation for arc in arcs]  # 提取依存关系\n",
    "    heads = ['Root' if id == 0 else words[id - 1] for id in rely_id]  # 匹配依存父节点词语\n",
    "    #找到谓语\n",
    "    for i in range(len(words)):\n",
    "        if relation[i]=='SBV' and heads[i] in dict:\n",
    "            return[words[i],heads[i]]\n",
    "    return None\n",
    "#\n",
    "def get_sbv2_(corpus,dict):\n",
    "    words = segmentor(corpus)\n",
    "    postags = posttagger(words)\n",
    "    arcs = parse(words,postags)\n",
    "    child_dict_list, format_parse_list = build_parse_child_dict(words, postags, arcs)\n",
    "    #找到谓语\n",
    "    for i in range(len(words)):\n",
    "        if format_parse_list[i][0]=='SBV' and format_parse_list[i][4] in dict:\n",
    "            full_sub= complete_e(words, postags, child_dict_list, i)\n",
    "            shuo_verb = format_parse_list[i][4]\n",
    "            return [full_sub,shuo_verb]\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def get_views(corpus):\n",
    "    dict = shuo_dict\n",
    "    sents=sentence_splitter(corpus)\n",
    "    views=[]\n",
    "    for sent in sents:\n",
    "        if sent and get_sbv1_(sent,dict):\n",
    "            view = get_sbv1_(sent,dict)+[sent]\n",
    "            views.append(view)\n",
    "    return views\n",
    "\n",
    "corpus1=\"\"\"\n",
    "昨日，雷先生说，交警部门罚了他 16 次，他只认了一次，交了一次罚款，拿到法\n",
    "院的判决书后，会前往交警队，要求撤销此前的处罚。\n",
    "\n",
    "律师：不依法粘贴告知单有谋取罚款之嫌。\n",
    "陕西金镝律师事务所律师骆裕德说，这起案件中，交警部门在处理交通违法的程\n",
    "序上存在问题。司机违停了，交警应将处罚单张贴在车上，并告知不服可以行使申请\n",
    "复议和提起诉讼的权利。这既是交警的告知义务，也是司机的知情权利。交警如果这\n",
    "么做了，本案司机何以被短时间内处罚 16 次后才知晓被罚？程序违法，为罚而罚，没\n",
    "有起到教育的目的。\n",
    "\"\"\"\n",
    "\n",
    "corpus2=\"\"\"\n",
    "中新网6月23日电 (记者潘旭临) 意大利航空首席商务官乔治先生22日在北京接受中新网记者专访时表示，\n",
    "意航确信中国市场对意航的重要性，目前意航已将发展中国市场提升到战略层级的高度，\n",
    "未来，意航将加大在华布局，提升业务水平。\n",
    "到意大利航空履职仅7个月的乔治，主要负责包括中国市场在内的亚太业务。\n",
    "乔治称，随着对华业务不断提升，意航明年可能会将每周4班提高到每天一班。同时，意航会借罗马新航站楼启用之际，\n",
    "吸引更多中国旅客到意大利旅游和转机。\n",
    "此外，还将加大对北京直飞航线的投资，如翻新航班座椅，\n",
    "增加电视中有关中国内容的娱乐节目、提高机上中文服务、餐饮服务、完善意航中文官方网站，\n",
    "提升商务舱和普通舱的舒适度等。\n",
    "\"\"\"\n",
    "\n",
    "def test(corpus):\n",
    "    views = get_views(corpus)\n",
    "    for view in views:\n",
    "        print(view)\n",
    "test(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
