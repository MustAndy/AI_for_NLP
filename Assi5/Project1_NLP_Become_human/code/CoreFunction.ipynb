{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\aicourse\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "d:\\anaconda3\\envs\\aicourse\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import re\n",
    "import jieba\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'd:/senior/aiCourse/dataSource/'\n",
    "MODEL_PATH = os.path.join(DATA_PATH,'w2vmodel/wiki_news_cutting.model')\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_PATH,'wiki_news_cutting.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我在代码外将wiki和news的语料库合并成了一个wiki_news_record.txt的文件，直接作为词向量的训练库输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(\"Model found ! Now loading...\")\n",
    "        return Word2Vec.load(MODEL_PATH)\n",
    "    else:\n",
    "        print(\"Model not found, start training, please wait for about 15 minutes.\")\n",
    "        model= Word2Vec( LineSentence(open(TRAIN_DATA_PATH,encoding='gb18030')), size=100,min_count=1, workers=3)\n",
    "        model.save(MODEL_PATH)\n",
    "        return model\n",
    "#下面直接加载与训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found ! Now loading...\n"
     ]
    }
   ],
   "source": [
    "all_word2vec = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import datetime\n",
    "\n",
    "@lru_cache(maxsize=10*100*100)\n",
    "def find_most_similar(node, topn_in ,model):\n",
    "    new_expanding_temp = [w for w,s in model.most_similar(node,topn=topn_in)]\n",
    "    return new_expanding_temp\n",
    "\n",
    "def get_related_words(initial_words, model):\n",
    "    \"\"\"\n",
    "    @initial_words are initial words we already know\n",
    "    @model is the word2vec model\n",
    "    \"\"\"\n",
    "    starttime = datetime.datetime.now()\n",
    "    \n",
    "    unseen = initial_words\n",
    "    \n",
    "    seen = defaultdict(int)\n",
    "    \n",
    "    max_size = 1000  # could be greater\n",
    "    \n",
    "    while unseen and len(seen) < max_size:\n",
    "        #if len(seen) % 50 == 0: \n",
    "        #    print('seen length : {}'.format(len(seen)))\n",
    "          \n",
    "        node = unseen.pop(0)\n",
    "                \n",
    "        if seen[node]:\n",
    "            if seen[node]>10:\n",
    "                seen[node] += seen[node]/50\n",
    "            else:\n",
    "                seen[node]+=1\n",
    "            new_expanding =  find_most_similar(node, 10 ,model)\n",
    "            unseen+=new_expanding\n",
    "            continue\n",
    "               \n",
    "        new_expanding =  find_most_similar(node, 10 ,model)  \n",
    "        unseen += new_expanding\n",
    "        seen[node] += 1\n",
    "        \n",
    "        # optimal: 1. score function could be revised\n",
    "        # optimal: 2. using dymanic programming to reduce computing time\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('Using : {} s'.format((endtime-starttime).seconds))\n",
    "    \n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_say_word():\n",
    "    say_word_file= os.path.join(DATA_PATH,'say.pickle')\n",
    "    if os.path.exists(say_word_file):  \n",
    "        print('Pickle file found ! Now loading!')\n",
    "        result = pickle.load(open(say_word_file,'rb'))\n",
    "    else:\n",
    "        print('Pickle file not found ! Starting searching, please wait for about 2 minutes.')\n",
    "        related_words = get_related_words(['说', '表示'], all_word2vec)\n",
    "        pickle.dump(related_words,open(say_word_file,'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "        result = pickle.load(open(say_word_file,'rb'))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file found ! Now loading!\n"
     ]
    }
   ],
   "source": [
    "say_word=load_say_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"新华社华盛顿4月26日电 美国总统特朗普26日表示，美国将撤销在《武器贸易条约》上的签字。\n",
    "\n",
    "特朗普当天在美国印第安纳州首府印第安纳波利斯举行的美国全国步枪协会年会上说，《武器贸易条约》是一个“严重误导的条约”，美国将撤销在该条约上的签字，联合国将很快收到美国正式拒绝该条约的通知。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"王近亲表示他今晚吃了十个包子\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"昨日，雷先生说，交警部门罚了他 16 次，他只认了一次，交了一次罚款，拿到法院的判决书后，会前往交警队，要求撤销此前的处罚。\n",
    "\n",
    "律师：不依法粘贴告知单有谋取罚款之嫌。\n",
    "陕西金镝律师事务所律师骆裕德说，这起案件中，交警部门在处理交通违法的程序上存在问题。司机违停了，交警应将处罚单张贴在车上，并告知不服可以行使申请\n",
    "复议和提起诉讼的权利。这既是交警的告知义务，也是司机的知情权利。交警如果这么做了，本案司机何以被短时间内处罚 16 次后才知晓被罚？程序违法，为罚而罚，没\n",
    "有起到教育的目的。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3= \"\"\"中新网6月23日电 (记者潘旭临) 意大利航空首席商务官乔治先生22日在北京接受中新网记者专访时表示，意航确信中国市场对意航的重要性，目前意航已将发展中国市场提升到战略层级的高度，\n",
    "未来，意航将加大在华布局，提升业务水平。\n",
    "到意大利航空履职仅7个月的乔治，主要负责包括中国市场在内的亚太业务。\n",
    "乔治称，随着对华业务不断提升，意航明年可能会将每周4班提高到每天一班。同时，意航会借罗马新航站楼启用之际，吸引更多中国旅客到意大利旅游和转机。\n",
    "此外，还将加大对北京直飞航线的投资，如翻新航班座椅，增加电视中有关中国内容的娱乐节目、提高机上中文服务、餐饮服务、完善意航中文官方网站，提升商务舱和普通舱的舒适度等。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.801 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'新华社 华盛顿 4 月 26 日电   美国 总统 特朗普 26 日 表示 ， 美国 将 撤销 在 武器 贸易 条约 上 的 签字 。 \\n \\n 特朗普 当天 在 美国 印第安纳州 首府 印第安纳波利斯 举行 的 美国 全国 步枪 协会 年 会上 说 ， 武器 贸易 条约 是 一个 严重 误导 的 条约 ， 美国 将 撤销 在 该 条约 上 的 签字 ， 联合国 将 很快 收到 美国 正式 拒绝 该 条约 的 通知 。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol='[^(（|）|《|》|“|”|「|」|、|\\-|：|·|；|?|,|.|:|\"|\\'|\\')]'\n",
    "#print(symbol)\n",
    "def token(string):\n",
    "    return ''.join(re.findall(symbol,string))\n",
    "\n",
    "cut(token(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "from pyltp import SementicRoleLabeller\n",
    "\n",
    "LTP_DATA_DIR = 'D://senior/aiCourse/dataSource/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyltp_cutting(sentence):\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(cws_model_path)  # 加载模型\n",
    "    result = segmentor.segment(sentence)  # 分词\n",
    "    #print ('\\t'.join(words))\n",
    "    segmentor.release()  # 释放模型\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyltp_postagger(words):\n",
    "    postagger = Postagger() # 初始化实例\n",
    "    postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "    result = postagger.postag(words)  # 词性标注\n",
    "\n",
    "   # print ('\\t'.join(postags))\n",
    "    postagger.release()  # 释放模型\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyltp_ner(words,postags):\n",
    "    recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "    recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "\n",
    "    result = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "\n",
    "    #print ('\\t'.join(netags))\n",
    "    recognizer.release()  # 释放模型\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyltp_parser(words,postags):\n",
    "    parser = Parser() # 初始化实例\n",
    "    parser.load(par_model_path)  # 加载模型\n",
    "    result = parser.parse(words, postags)  # 句法分析\n",
    "\n",
    "    #print (\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "    parser.release()  # 释放模型\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyltp_role_parsring(words, postags, arcs):\n",
    "    labeller = SementicRoleLabeller() # 初始化实例\n",
    "    labeller.load(srl_model_path)  # 加载模型\n",
    "    result = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "    #for role in roles:\n",
    "    #    print (words[role.index], \"\".join(\n",
    "    #        [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "    labeller.release()  # 释放模型\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=pyltp_cutting(cut(token(text3)))\n",
    "\n",
    "postags=pyltp_postagger(words)\n",
    "netags = pyltp_ner(words,postags)\n",
    "arcs = pyltp_parser(words,postags)\n",
    "roles = pyltp_role_parsring(words, postags, arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表示 A0:(4,11)TMP:(12,19)A1:(22,57)\n",
      "确信 A0:(22,22)A1:(24,29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for role in roles:\n",
    "    if words[role.index] in say_word.keys():\n",
    "        if say_word[words[role.index]]>5:\n",
    "            print (words[role.index], \"\".join(\n",
    "        [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = []\n",
    "verb=[]\n",
    "contents=[]\n",
    "for role in roles:\n",
    "    if words[role.index] in say_word.keys():\n",
    "        if say_word[words[role.index]]>5:\n",
    "            parsing_argument = []\n",
    "            for arg in role.arguments:\n",
    "                parsing_argument.append(arg.name)\n",
    "            if 'A0' in parsing_argument and 'A1' in parsing_argument:\n",
    "                verb.append ( words[role.index])\n",
    "                for arg in role.arguments:\n",
    "                    if arg.name =='A0':\n",
    "                        people.append(''.join(words[i] for i in range(arg.range.start, arg.range.end+1)))\n",
    "                    if arg.name =='A1':\n",
    "                        contents.append(''.join(words[i] for i in range(arg.range.start, arg.range.end+1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['记者潘旭临意大利航空首席商务官乔治先生',\n",
       "  '表示',\n",
       "  '意航确信中国市场对意航的重要性，目前意航已将发展中国市场提升到战略层级的高度，\\n未来，意航将加大在华布局，提升业务水平'],\n",
       " ['意航', '确信', '中国市场对意航的重要性']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table = []\n",
    "for i in range(len(people)):\n",
    "    table.append([people[i],verb[i],contents[i]])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中新网 n O \n",
      "6月 nt O \n",
      "23日 nt O \n",
      "电 n O \n",
      "记者 n O \n",
      "潘旭临 nh S-Nh \n",
      "意大利 ns S-Ns \n",
      "航空 n O \n",
      "首席 n O \n",
      "商务官 n O \n",
      "乔治 nh S-Nh \n",
      "先生 n O \n",
      "22日 nt O \n",
      "在 p O \n",
      "北京 ns S-Ns \n",
      "接受 v O \n",
      "中新网 nz O \n",
      "记者 n O \n",
      "专访 v O \n",
      "时 n O \n",
      "表示 v O \n",
      "， wp O \n",
      "意航 j O \n",
      "确信 v O \n",
      "中国 ns S-Ns \n",
      "市场 n O \n",
      "对 p O \n",
      "意航 j O \n",
      "的 u O \n",
      "重要性 n O \n",
      "， wp O \n",
      "目前 nt O \n",
      "意航 j O \n",
      "已 d O \n",
      "将 p O \n",
      "发展 v O \n",
      "中国 ns S-Ns \n",
      "市场 n O \n",
      "提升 v O \n",
      "到 v O \n",
      "战略 n O \n",
      "层级 n O \n",
      "的 u O \n",
      "高度 n O \n",
      "， wp O \n",
      "\n",
      " v O \n",
      "未来 nt O \n",
      "， wp O \n",
      "意航 j O \n",
      "将 d O \n",
      "加大 v O \n",
      "在 p O \n",
      "华 j O \n",
      "布局 n O \n",
      "， wp O \n",
      "提升 v O \n",
      "业务 n O \n",
      "水平 n O \n",
      "。 wp O \n",
      "\n",
      "到 v O \n",
      "意大利 ns S-Ns \n",
      "航空 n O \n",
      "履职 n O \n",
      "仅 d O \n",
      "7 m O \n",
      "个 q O \n",
      "月 n O \n",
      "的 u O \n",
      "乔治 nh S-Nh \n",
      "， wp O \n",
      "主要 d O \n",
      "负责 v O \n",
      "包括 v O \n",
      "中国 ns S-Ns \n",
      "市场 n O \n",
      "在内 u O \n",
      "的 u O \n",
      "亚 j O \n",
      "太 j O \n",
      "业务 n O \n",
      "。 wp O \n",
      "\n",
      " n O \n",
      "乔治 nh S-Nh \n",
      "称 v O \n",
      "， wp O \n",
      "随着 p O \n",
      "对 p O \n",
      "华 j O \n",
      "业务 n O \n",
      "不断 d O \n",
      "提升 v O \n",
      "， wp O \n",
      "意航 j O \n",
      "明年 nt O \n",
      "可能 v O \n",
      "会 v O \n",
      "将 p O \n",
      "每周 r O \n",
      "4 m O \n",
      "班 n O \n",
      "提高 v O \n",
      "到 v O \n",
      "每天 r O \n",
      "一 m O \n",
      "班 q O \n",
      "。 wp O \n",
      "同时 n O \n",
      "， wp O \n",
      "意航 j O \n",
      "会 v O \n",
      "借 v O \n",
      "罗马 ns B-Ni \n",
      "新航 j E-Ni \n",
      "站楼 n O \n",
      "启用 v O \n",
      "之际 nd O \n",
      "， wp O \n",
      "吸引 v O \n",
      "更 d O \n",
      "多 a O \n",
      "中国 ns S-Ns \n",
      "旅客 n O \n",
      "到 v O \n",
      "意大利 ns S-Ns \n",
      "旅游 v O \n",
      "和 c O \n",
      "转机 n O \n",
      "。 wp O \n",
      "\n",
      " n O \n",
      "此外 c O \n",
      "， wp O \n",
      "还 d O \n",
      "将 d O \n",
      "加大 v O \n",
      "对 p O \n",
      "北京 ns S-Ns \n",
      "直 d O \n",
      "飞 v O \n",
      "航线 n O \n",
      "的 u O \n",
      "投资 v O \n",
      "， wp O \n",
      "如 v O \n",
      "翻新 v O \n",
      "航班 n O \n",
      "座椅 n O \n",
      "， wp O \n",
      "增加 v O \n",
      "电视 n O \n",
      "中 nd O \n",
      "有关 p O \n",
      "中国 ns S-Ns \n",
      "内容 n O \n",
      "的 u O \n",
      "娱乐 v O \n",
      "节目 n O \n",
      "提高 v O \n",
      "机上 nl O \n",
      "中文 nz O \n",
      "服务 v O \n",
      "餐饮 n O \n",
      "服务 v O \n",
      "完善 v O \n",
      "意航 j O \n",
      "中文 nz O \n",
      "官方 n O \n",
      "网站 n O \n",
      "， wp O \n",
      "提升 v O \n",
      "商务舱 n O \n",
      "和 c O \n",
      "普通舱 n O \n",
      "的 u O \n",
      "舒适度 n O \n",
      "等 u O \n",
      "。 wp O \n"
     ]
    }
   ],
   "source": [
    "length = len(words)\n",
    "for i in range(length):\n",
    "    print('{} {} {} '.format(words[i],postags[i],netags[i]))\n",
    "    #if postags[i]=='nh' or postags[i]=='nz':\n",
    "    #if netags[i]!='O':\n",
    "     #   print('{} {} {} '.format(words[i],postags[i],netags[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'新华社 华盛顿 4 月 26 日电   美国 总统 特朗普 26 日 表示 ， 美国 将 撤销 在 武器 贸易 条约 上 的 签字 。 \\n \\n 特朗普 当天 在 美国 印第安纳州 首府 印第安纳波利斯 举行 的 美国 全国 步枪 协会 年 会上 说 ， 武器 贸易 条约 是 一个 严重 误导 的 条约 ， 美国 将 撤销 在 该 条约 上 的 签字 ， 联合国 将 很快 收到 美国 正式 拒绝 该 条约 的 通知 。'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut(token(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(word): \n",
    "    return sum(1 for n in news_content if word in n)\n",
    "\n",
    "def idf(word):\n",
    "    \"\"\"Gets the inversed document frequency\"\"\"\n",
    "    return math.log10(len(news_content) / document_frequency(word))\n",
    "\n",
    "def tf(word, document):\n",
    "    \"\"\"\n",
    "    Gets the term frequemcy of a @word in a @document.\n",
    "    \"\"\"\n",
    "    document=cut(token(document))\n",
    "    words = document.split()\n",
    "    \n",
    "    return sum(1 for w in words if w == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf('意大利',text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
