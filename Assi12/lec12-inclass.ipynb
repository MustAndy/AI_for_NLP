{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.435570Z",
     "start_time": "2019-07-06T10:46:57.315041Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.450669Z",
     "start_time": "2019-07-06T10:46:57.440303Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,inputs=[]):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            \n",
    "        self.value = None\n",
    "        self.gradients = {\n",
    "            #if is wx+b,this will put the \n",
    "            # x:x\n",
    "            # x:w\n",
    "            # b:l\n",
    "        }\n",
    "        \n",
    "    def forward_partial(self):\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.466452Z",
     "start_time": "2019-07-06T10:46:57.455683Z"
    }
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    #输入结点，前向传播就是简单的将值递出去\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward_partial(self,value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self]=grad_cost*1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.486304Z",
     "start_time": "2019-07-06T10:46:57.470605Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self,nodes,weights,bias):\n",
    "        Node.__init__(self,[nodes,weights,bias])\n",
    "        \n",
    "        \n",
    "    def forward_partial(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights=self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "        #print(self.inputs[0].value)\n",
    "\n",
    "        self.value = np.dot(inputs,weights)+bias\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        self.gradients = {n:np.zeros_like(n.value) for n in self.inputs}\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost , self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] =np.sum(grad_cost,axis=0,keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.505456Z",
     "start_time": "2019-07-06T10:46:57.490787Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self,node):\n",
    "        Node.__init__(self,[node])\n",
    "        \n",
    "    def _sigmoid(self,x):\n",
    "        return 1./(1+np.exp(-1*x))\n",
    "    \n",
    "    def forward_partial(self):\n",
    "        self.x=self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        self.partial = self._sigmoid(self.x)*(1-self._sigmoid(self.x))\n",
    "        \n",
    "        self.gradients = {n:np.zeros_like(n.value) for n in self.inputs}\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            self.gradients[self.inputs[0]] = grad_cost*self.partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.526151Z",
     "start_time": "2019-07-06T10:46:57.509450Z"
    }
   },
   "outputs": [],
   "source": [
    "class LOSS(Node):\n",
    "    def __init__(self, y_true, y_hat):\n",
    "        Node.__init__(self,[y_true,y_hat])\n",
    "        \n",
    "    def forward_partial(self):\n",
    "        #拉平数组\n",
    "        y_true = self.inputs[0].value.reshape(-1,1)\n",
    "        y_hat = self.inputs[1].value.reshape(-1,1)\n",
    "        \n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff=y_true-y_hat\n",
    "        \n",
    "        self.value = np.mean(self.diff **2)\n",
    "        \n",
    "    def backward_partial(self):      \n",
    "        \n",
    "        self.gradients[self.inputs[0]] = (2/self.m)*self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2/self.m)*self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.538049Z",
     "start_time": "2019-07-06T10:46:57.531250Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_one_epoch(output_node,graph):\n",
    "    for n in graph:\n",
    "        n.forward_partial()\n",
    "        \n",
    "    for n in graph[::-1]:\n",
    "        n.backward_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.563827Z",
     "start_time": "2019-07-06T10:46:57.542799Z"
    }
   },
   "outputs": [],
   "source": [
    "#从图入度为0的点开始抽取，排序整张图的节点\n",
    "def toplogical_sort(graph):\n",
    "    \n",
    "    input_nodes = [n for n in graph.keys()]\n",
    "    \n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    \n",
    "    while len(nodes)>0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n]={'in':set(),'out':set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m]={'in':set(),'out':set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    \n",
    "    while len(S)>0:\n",
    "        n = S.pop()\n",
    "        if isinstance(n,Input):\n",
    "            n.value = graph[n]\n",
    "        \n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            \n",
    "            if len(G[m]['in'])==0:\n",
    "                S.add(m)\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:57.576987Z",
     "start_time": "2019-07-06T10:46:57.568601Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(trainable_nodes,learning_rate = 1e-3):\n",
    "    for node in trainable_nodes:\n",
    "        node.value += -1*(learning_rate * node.gradients[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.436917Z",
     "start_time": "2019-07-06T10:46:57.582278Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.456973Z",
     "start_time": "2019-07-06T10:46:58.436917Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.471885Z",
     "start_time": "2019-07-06T10:46:58.459663Z"
    }
   },
   "outputs": [],
   "source": [
    "#load_data\n",
    "X_=data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "#normolize\n",
    "X_ = (X_ -np.mean(X_,axis=0))/np.std(X_,axis = 0)\n",
    "n_features =X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features,n_hidden)\n",
    "b1_=np.zeros(n_hidden)\n",
    "W2_=np.random.randn(n_hidden,1)\n",
    "b2_= np.zeros(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.484130Z",
     "start_time": "2019-07-06T10:46:58.474935Z"
    }
   },
   "outputs": [],
   "source": [
    "X,y=Input(),Input()\n",
    "W1,b1 = Input(),Input()\n",
    "W2,b2 = Input(),Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.496172Z",
     "start_time": "2019-07-06T10:46:58.488149Z"
    }
   },
   "outputs": [],
   "source": [
    "linear1 = Linear(X,W1,b1)\n",
    "sigmoid_1 = Sigmoid(linear1)\n",
    "linear2 = Linear(sigmoid_1,W2,b2)\n",
    "cost = LOSS(y,linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.508474Z",
     "start_time": "2019-07-06T10:46:58.499968Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample,shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.525450Z",
     "start_time": "2019-07-06T10:46:58.510833Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = {\n",
    "    X:X_,\n",
    "    y:y_,\n",
    "    W1:W1_,\n",
    "    b1:b1_,\n",
    "    W2:W2_,\n",
    "    b2:b2_    \n",
    "}\n",
    "\n",
    "graph = toplogical_sort(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:46:58.540438Z",
     "start_time": "2019-07-06T10:46:58.531297Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 16\n",
    "batch_num = X_.shape[0]//batch_size\n",
    "trainables = [W1, b1, W2, b2]\n",
    "rate = 1e-2\n",
    "losses=[]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:47:10.089771Z",
     "start_time": "2019-07-06T10:46:58.543985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 155.889\n",
      "Epoch: 20, Loss: 10.336\n",
      "Epoch: 40, Loss: 8.618\n",
      "Epoch: 60, Loss: 9.435\n",
      "Epoch: 80, Loss: 7.583\n",
      "Epoch: 100, Loss: 7.697\n",
      "Epoch: 120, Loss: 6.365\n",
      "Epoch: 140, Loss: 5.333\n",
      "Epoch: 160, Loss: 5.240\n",
      "Epoch: 180, Loss: 6.969\n",
      "Epoch: 200, Loss: 5.739\n",
      "Epoch: 220, Loss: 5.893\n",
      "Epoch: 240, Loss: 6.061\n",
      "Epoch: 260, Loss: 5.245\n",
      "Epoch: 280, Loss: 5.053\n",
      "Epoch: 300, Loss: 4.273\n",
      "Epoch: 320, Loss: 4.790\n",
      "Epoch: 340, Loss: 4.950\n",
      "Epoch: 360, Loss: 4.519\n",
      "Epoch: 380, Loss: 4.503\n",
      "Epoch: 400, Loss: 4.537\n",
      "Epoch: 420, Loss: 4.361\n",
      "Epoch: 440, Loss: 3.963\n",
      "Epoch: 460, Loss: 3.420\n",
      "Epoch: 480, Loss: 4.350\n",
      "Epoch: 500, Loss: 3.801\n",
      "Epoch: 520, Loss: 4.447\n",
      "Epoch: 540, Loss: 4.102\n",
      "Epoch: 560, Loss: 4.088\n",
      "Epoch: 580, Loss: 3.717\n",
      "Epoch: 600, Loss: 3.325\n",
      "Epoch: 620, Loss: 3.673\n",
      "Epoch: 640, Loss: 3.624\n",
      "Epoch: 660, Loss: 3.827\n",
      "Epoch: 680, Loss: 3.823\n",
      "Epoch: 700, Loss: 3.254\n",
      "Epoch: 720, Loss: 3.744\n",
      "Epoch: 740, Loss: 3.166\n",
      "Epoch: 760, Loss: 3.811\n",
      "Epoch: 780, Loss: 3.444\n",
      "Epoch: 800, Loss: 3.201\n",
      "Epoch: 820, Loss: 3.847\n",
      "Epoch: 840, Loss: 3.286\n",
      "Epoch: 860, Loss: 3.764\n",
      "Epoch: 880, Loss: 3.357\n",
      "Epoch: 900, Loss: 2.984\n",
      "Epoch: 920, Loss: 3.573\n",
      "Epoch: 940, Loss: 3.316\n",
      "Epoch: 960, Loss: 3.488\n",
      "Epoch: 980, Loss: 3.524\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    \n",
    "    for batch in range(batch_num):\n",
    "        X_batch,y_batch = resample(X_,y_,n_samples = batch_size)\n",
    "        \n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "    \n",
    "        finally_output = None\n",
    "        \n",
    "        run_one_epoch(finally_output,graph)\n",
    "        \n",
    "        gradient_descent_update(trainables,rate)\n",
    "        \n",
    "        loss +=graph[-1].value\n",
    "        #print(loss)    \n",
    "    if epoch % 20 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(epoch, loss/batch_num))\n",
    "        losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
